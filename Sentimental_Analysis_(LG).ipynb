{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OZBGG-hyELA"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QgPf6Je7ahV"
      },
      "outputs": [],
      "source": [
        "import nltk, re, string\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eggLpP7VyIKA"
      },
      "source": [
        "Pre-Processing of the tweets (Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdNcabhLx0J5"
      },
      "outputs": [],
      "source": [
        "def process_tweet(tweet):\n",
        "  stemmer = nltk.PorterStemmer()\n",
        "  stopwords_english = stopwords.words(\"english\")\n",
        "  tweet = re.sub(r'\\$\\w', '', tweet)\n",
        "  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "  tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "  tweet = re.sub(r'#', '', tweet)\n",
        "  tokenizer = nltk.TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "  tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "  tweet_clean=[]\n",
        "  for word in tweet_tokens:\n",
        "    if(word not in stopwords_english and\n",
        "       word not in string.punctuation):\n",
        "      stem_word = stemmer.stem(word)\n",
        "      tweet_clean.append(stem_word)\n",
        "\n",
        "  return tweet_clean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY-ifsOeyP-6"
      },
      "source": [
        "In the preprocessing following things happen:\n",
        "1. re function removes all the unnecessary number, links, special char etc. from the tweets\n",
        "2. tokenizer funct takes string as an input and return the list of words.\n",
        "3. a loop is used for further cleaning by using the stemmer function which return the base words of the string eg.(eating --> eat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKMgQRm5x2Rb"
      },
      "outputs": [],
      "source": [
        "def build_freqs(tweets, ys):\n",
        "  #convert np array ito lists since zip needs an iterable\n",
        "  #the sqeeze function is necessary or the list ends up with one element\n",
        "  #Also note that this is just a NOP is ys is already a list\n",
        "  yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "  #start with an empty dictionary and populate it by looping over all tweets\n",
        "  # and over all processed words in each tweets.\n",
        "\n",
        "  freqs={}\n",
        "  for y, tweet in zip(yslist, tweets):\n",
        "    for word in process_tweet(tweet):\n",
        "      pair = (word, y)\n",
        "      if pair in freqs:\n",
        "        freqs[pair] +=1\n",
        "      else:\n",
        "        freqs[pair] =1\n",
        "  return freqs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZEB_HyxyUar"
      },
      "source": [
        "the build_freq() function contains a empty dictionary which will contain all the words with there class lable \"positive\" or \"negetive\" along with the no. of times it occured. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaynSb1bx40t",
        "outputId": "b55d9110-9bbf-4f5f-e3c8-ae21922ad1fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(freqs) =<class 'dict'>\n",
            "len(freqs) =11339\n"
          ]
        }
      ],
      "source": [
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n",
        "freqs = build_freqs(train_x, train_y)\n",
        "\n",
        "print('type(freqs) =' + str(type(freqs)))\n",
        "print('len(freqs) =' + str(len(freqs.keys())))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjzFQ9VJyblW"
      },
      "source": [
        "In this cell the data set are being prepared for the training and testing purpose \n",
        "1. all_positive_tweets and all_negative_tweets variable store the respective tweets.\n",
        "2. test and train positive and negative data are divided \n",
        "3. x variable of training data if both postive and negative is initialized \n",
        "4. y-target variable is initialized by combining positive and negative labels\n",
        "5. freqs dictionary is been created using build_freqs function.\n",
        "6. type and length is printed \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDiPkjz32r0Y"
      },
      "source": [
        "# **Model building** - Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "141hqS3ax7Rc"
      },
      "outputs": [],
      "source": [
        "# sigmoid function\n",
        "def sigmoid(z):\n",
        "  zz= np.negative(z)\n",
        "  h = 1 / (1 + np.exp(zz))\n",
        "  return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf2NREUh4N6i"
      },
      "outputs": [],
      "source": [
        "#cost function and Gradient\n",
        "\n",
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "  '''\n",
        "  Input:\n",
        "        x: matrix of the features which is (m+1)\n",
        "        y: corresponding labels of the input matrix x, dimension (m,1)\n",
        "        theta: weight vector of dimension (n+1, 1):\n",
        "        alpha: learning rate\n",
        "        num_iters: number of interations you want to train your model for\n",
        "  Output:\n",
        "        J: final cost\n",
        "        theta: your final weight vector\n",
        "  '''\n",
        "  #get 'm', the number of rows in matrix x\n",
        "  m= x.shape[0]\n",
        "  for i in range(0, num_iters):\n",
        "    z= np.dot(x,theta)\n",
        "    h = sigmoid(z)\n",
        "    #calculate cost func\n",
        "    cost = -1/m * (np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(), np.log(1-h)))\n",
        "    #update the weight theta\n",
        "    theta = theta - (alpha/m) * np.dot(x.transpose(), (h-y))\n",
        "  cost=float(cost)\n",
        "  return cost, theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rihuo1gSbIwv"
      },
      "source": [
        "Feature Extraction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxU5iuym5MTp"
      },
      "outputs": [],
      "source": [
        "def extract_features(tweet, freqs):\n",
        "  '''\n",
        "  Input:\n",
        "        tweet: a list of words for one tweet\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple(word, label)\n",
        "  Output:\n",
        "        x: a feature vector of dimension (1,3)\n",
        "  '''\n",
        "  word_l = process_tweet(tweet)\n",
        "  x=np.zeros((1,3))\n",
        "  #bias term is set to 1\n",
        "  x[0,0] = 1\n",
        "  for word in word_l:\n",
        "    #increment the word count for the positive label 1\n",
        "    x[0,1] += freqs.get((word, 1.0), 0)\n",
        "    #increment the word count for the negative label 0\n",
        "    x[0,2] += freqs.get((word, 0.0), 0)\n",
        "\n",
        "  assert (x.shape ==(1,3))\n",
        "  return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qrwos6nmdCTC"
      },
      "source": [
        "Testing training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUSvGH5Xc7fu",
        "outputId": "ff5034aa-c51f-4681-a7ef-2aa5210262ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.000e+00 3.006e+03 1.240e+02]]\n"
          ]
        }
      ],
      "source": [
        "tmp1 = extract_features(train_x[22], freqs)\n",
        "print(tmp1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdF27q7JQsaF"
      },
      "source": [
        "Testing training data explaination:\n",
        "-> Usually we get a dataset with lot of features/columns, here we just have text data.\n",
        "-> those 3 numbers are the feature set that we have built using build_freqs() and extract_feature() function.\n",
        "-> bulid_freq() builts a dictionary having words as keys and number of times they have occured in corpus values.\n",
        "-> Extract feature takes in sum of these values for positive and negetive words, i.e. tmp1[1] and tmp[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSW4UDG0R1LM"
      },
      "source": [
        "How this feature will be used to predict the losgistic Regression:\n",
        "-> First a hypothesis is buiold in which for our case will be h(x) = b1 + b2*x1 + b3*x2\n",
        "-> here b1=1, b2 and b3 are determined by cost and gradient descent function, x1 and x2 are the positive and negetive words feature set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBoyw9trQKVJ"
      },
      "outputs": [],
      "source": [
        "#Training the model\n",
        "\n",
        "#collect the features \"x\" and stack them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "  X[i, :]= extract_features(train_x[i], freqs)\n",
        "\n",
        "#training labels corresponding to X\n",
        "Y = train_y\n",
        "\n",
        "#Apply gradient descent \n",
        "#these values are predefined\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3,1)), 1e-9, 1500)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2AV3Q7HUNJp"
      },
      "outputs": [],
      "source": [
        "def predict_tweet(tweet, freqs, theta):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "        tweet: a string \n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights \n",
        "  Output:\n",
        "        y_pred: the probablity of a tweet being positive or negetive \n",
        "  \"\"\"\n",
        "  # extract the feature of the tweet and store it into x\n",
        "  x= extract_features(tweet, freqs)\n",
        "  y_pred= sigmoid(np.dot(x, theta))\n",
        "\n",
        "  return y_pred\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MNDGatI8sMt"
      },
      "outputs": [],
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "        test_x: a list of tweets\n",
        "        test_y: (n,1) vector with the coresponding label for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3,1)\n",
        "  Output:\n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "  \"\"\"\n",
        "  # list for storing predictions\n",
        "  y_hat=[]\n",
        "\n",
        "  for tweet in test_x:\n",
        "    #get the label prediction for the tweet\n",
        "    y_pred = predict_tweet(tweet, freqs, theta)\n",
        "    if y_pred > 0.5:\n",
        "      y_hat.append(1)\n",
        "    else:\n",
        "      y_hat.append(0)\n",
        "  accuracy = (y_hat == np.squeeze(test_y)).sum() / len(test_x)\n",
        "\n",
        "  return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebQsHGaO-wvG",
        "outputId": "ade0cce9-8a54-48ae-886a-3cf5fcb0333c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression model's accuracy =  0.9950\n"
          ]
        }
      ],
      "source": [
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy: .4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8V1l8lKi_SBo"
      },
      "outputs": [],
      "source": [
        "#prediction with own tweet\n",
        "def pre(sentence):\n",
        "  yhat= predict_tweet(sentence, freqs, theta)\n",
        "  if yhat> 0.5:\n",
        "    return \"Positive statement\"\n",
        "  elif yhat == 0:\n",
        "    return \"Neutral statement\"\n",
        "  else:\n",
        "    return \"Negative statement\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXO7jdfEH4Qz",
        "outputId": "6fea4a27-0a10-44b1-e922-69c4a432240d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative statement\n"
          ]
        }
      ],
      "source": [
        "my_tweet = 'Riya is a very bad Girl'\n",
        "res = pre(my_tweet)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkV-mFmTIQNb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}